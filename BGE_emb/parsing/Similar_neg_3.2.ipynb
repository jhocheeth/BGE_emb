{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c286d5e0",
   "metadata": {},
   "source": [
    "Create pseudo random negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b7df9-83fa-4e66-bc87-30729c365113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from typing import Dict, List, Set\n",
    "import multiprocessing as mp\n",
    "from itertools import islice\n",
    "import os\n",
    "\n",
    "# Number of negatives to pick based on suffix similarity\n",
    "NUM_NEG_SAMPLES_SUFFIX = 2\n",
    "\n",
    "# Number of negatives to pick based on prefix similarity\n",
    "NUM_NEG_SAMPLES_PREFIX = 2\n",
    "\n",
    "# Total number of negatives to pick\n",
    "TOTAL_NUM_NEGATIVES = 8\n",
    "\n",
    "def chunk_file(filename: str, chunk_size: int = 1000):\n",
    "    \"\"\"Generator to read file in chunks.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        while True:\n",
    "            lines = list(islice(f, chunk_size))\n",
    "            if not lines:\n",
    "                break\n",
    "            yield lines\n",
    "\n",
    "def safe_get_last_word(text: str) -> str:\n",
    "    \"\"\"Safely get the last word of a string.\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    words = text.split()\n",
    "    return words[-1].lower() if words else \"\"\n",
    "\n",
    "def get_valid_candidates(candidates: List[str], pos: str, pos_last_word: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Suffix-based filter:\n",
    "      - Candidate != pos\n",
    "      - Candidate's last word != pos's last word\n",
    "    \"\"\"\n",
    "    return [\n",
    "        p for p in candidates\n",
    "        if p != pos and safe_get_last_word(p) != pos_last_word\n",
    "    ]\n",
    "\n",
    "def get_valid_candidates_prefix(\n",
    "    candidates: List[str], pos: str, prefix_length: int\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Prefix-based filter:\n",
    "      - Candidate != pos\n",
    "      - Candidate has at least 'prefix_length' letters\n",
    "      - Same first 'prefix_length' letters\n",
    "      - The (prefix_length+1)th letter is different from pos's (prefix_length+1)th letter, if both exist\n",
    "    \"\"\"\n",
    "    valid = []\n",
    "    if len(pos) < prefix_length:\n",
    "        return valid\n",
    "    \n",
    "    pos_prefix = pos[:prefix_length]\n",
    "    \n",
    "    # The next character in pos, if it exists\n",
    "    next_char = pos[prefix_length] if len(pos) >= prefix_length + 1 else None\n",
    "    \n",
    "    for c in candidates:\n",
    "        if c == pos:\n",
    "            continue\n",
    "        if len(c) < prefix_length:\n",
    "            continue\n",
    "        if c[:prefix_length] != pos_prefix:\n",
    "            continue\n",
    "        \n",
    "        # Check that the (prefix_length+1)th character is different (if both exist)\n",
    "        if next_char is not None and len(c) >= prefix_length + 1:\n",
    "            if c[prefix_length] == next_char:\n",
    "                continue\n",
    "        \n",
    "        valid.append(c)\n",
    "    return valid\n",
    "\n",
    "def process_chunk(chunk: List[str]) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Process a chunk of lines. We do:\n",
    "      1) Build local suffix and prefix dictionaries\n",
    "      2) For each entry, pick:\n",
    "         a) 2 suffix-based negatives\n",
    "         b) 2 prefix-based negatives\n",
    "         c) Random negatives to make the total negatives 7\n",
    "    \"\"\"\n",
    "    local_suffix_dict_5 = defaultdict(set)\n",
    "    local_suffix_dict_4 = defaultdict(set)\n",
    "    local_suffix_dict_3 = defaultdict(set)\n",
    "    \n",
    "    # Build prefix dictionaries for lengths 4, 3, 2, 1\n",
    "    local_prefix_dict_4 = defaultdict(set)\n",
    "    local_prefix_dict_3 = defaultdict(set)\n",
    "    local_prefix_dict_2 = defaultdict(set)\n",
    "    local_prefix_dict_1 = defaultdict(set)\n",
    "    \n",
    "    # Keep track of ALL pos in the chunk for purely random negatives\n",
    "    all_positions_in_chunk = set()\n",
    "\n",
    "    # ========== 1) Build dictionaries ==========\n",
    "    entries = []\n",
    "    for line in chunk:\n",
    "        entry = json.loads(line.strip().replace(\"'\", '\"'))\n",
    "        entries.append(entry)\n",
    "        \n",
    "        for pos in entry.get('pos', []):\n",
    "            if not pos:\n",
    "                continue\n",
    "            # Track all positions for random negatives\n",
    "            all_positions_in_chunk.add(pos)\n",
    "            \n",
    "            # ========== Suffix logic ==========\n",
    "            if len(pos) >= 3:\n",
    "                suffix_3 = pos[-3:]\n",
    "                local_suffix_dict_3[suffix_3].add(pos)\n",
    "            if len(pos) >= 4:\n",
    "                suffix_4 = pos[-4:]\n",
    "                local_suffix_dict_4[suffix_4].add(pos)\n",
    "            if len(pos) >= 5:\n",
    "                suffix_5 = pos[-5:]\n",
    "                local_suffix_dict_5[suffix_5].add(pos)\n",
    "            \n",
    "            # ========== Prefix logic ==========\n",
    "            if len(pos) >= 1:\n",
    "                local_prefix_dict_1[pos[:1]].add(pos)\n",
    "            if len(pos) >= 2:\n",
    "                local_prefix_dict_2[pos[:2]].add(pos)\n",
    "            if len(pos) >= 3:\n",
    "                local_prefix_dict_3[pos[:3]].add(pos)\n",
    "            if len(pos) >= 4:\n",
    "                local_prefix_dict_4[pos[:4]].add(pos)\n",
    "    \n",
    "    # Convert sets to lists\n",
    "    suffix_dict_3 = {k: list(v) for k, v in local_suffix_dict_3.items()}\n",
    "    suffix_dict_4 = {k: list(v) for k, v in local_suffix_dict_4.items()}\n",
    "    suffix_dict_5 = {k: list(v) for k, v in local_suffix_dict_5.items()}\n",
    "\n",
    "    prefix_dict_4 = {k: list(v) for k, v in local_prefix_dict_4.items()}\n",
    "    prefix_dict_3 = {k: list(v) for k, v in local_prefix_dict_3.items()}\n",
    "    prefix_dict_2 = {k: list(v) for k, v in local_prefix_dict_2.items()}\n",
    "    prefix_dict_1 = {k: list(v) for k, v in local_prefix_dict_1.items()}\n",
    "\n",
    "    # A list for random picks\n",
    "    all_positions_list = list(all_positions_in_chunk)\n",
    "    \n",
    "    # ========== 2) Find negative samples for each entry ==========\n",
    "    updated_entries = []\n",
    "    for entry in entries:\n",
    "        pos_list = entry.get('pos', [])\n",
    "        \n",
    "        # We'll store all negatives (suffix-based + prefix-based + random) here\n",
    "        neg_samples_all = []\n",
    "        \n",
    "        # ========== (a) SUFFIX-BASED NEGATIVES (2) ==========\n",
    "        suffix_neg_samples = []\n",
    "        for pos in pos_list:\n",
    "            # We skip if too short for suffix logic\n",
    "            if not pos or len(pos) < 3:\n",
    "                continue\n",
    "            \n",
    "            pos_last_word = safe_get_last_word(pos)\n",
    "            all_candidates_suff = []\n",
    "            \n",
    "            # 1. Try suffix_5\n",
    "            if len(pos) >= 5:\n",
    "                suffix_5_part = pos[-5:]\n",
    "                candidates = get_valid_candidates(\n",
    "                    suffix_dict_5.get(suffix_5_part, []), pos, pos_last_word\n",
    "                )\n",
    "                all_candidates_suff.extend(candidates)\n",
    "            \n",
    "            # 2. Suffix_4 if still needed\n",
    "            if len(all_candidates_suff) < NUM_NEG_SAMPLES_SUFFIX and len(pos) >= 4:\n",
    "                suffix_4_part = pos[-4:]\n",
    "                candidates = get_valid_candidates(\n",
    "                    suffix_dict_4.get(suffix_4_part, []), pos, pos_last_word\n",
    "                )\n",
    "                all_candidates_suff.extend(candidates)\n",
    "            \n",
    "            # 3. Suffix_3 if still needed\n",
    "            if len(all_candidates_suff) < NUM_NEG_SAMPLES_SUFFIX and len(pos) >= 3:\n",
    "                suffix_3_part = pos[-3:]\n",
    "                candidates = get_valid_candidates(\n",
    "                    suffix_dict_3.get(suffix_3_part, []), pos, pos_last_word\n",
    "                )\n",
    "                all_candidates_suff.extend(candidates)\n",
    "            \n",
    "            # Choose up to 2 from suffix-based\n",
    "            selected_suff_neg = random.sample(\n",
    "                all_candidates_suff, \n",
    "                min(NUM_NEG_SAMPLES_SUFFIX, len(all_candidates_suff))\n",
    "            )\n",
    "            suffix_neg_samples.extend(selected_suff_neg)\n",
    "        \n",
    "        # ========== (b) PREFIX-BASED NEGATIVES (2) ==========\n",
    "        prefix_neg_samples = []\n",
    "        for pos in pos_list:\n",
    "            if not pos:\n",
    "                continue\n",
    "            \n",
    "            all_candidates_pref = []\n",
    "            \n",
    "            # 1. Prefix_4\n",
    "            if len(pos) >= 4:\n",
    "                prefix_4_part = pos[:4]\n",
    "                candidates = get_valid_candidates_prefix(\n",
    "                    prefix_dict_4.get(prefix_4_part, []), pos, 4\n",
    "                )\n",
    "                all_candidates_pref.extend(candidates)\n",
    "            \n",
    "            # 2. Prefix_3\n",
    "            if len(all_candidates_pref) < NUM_NEG_SAMPLES_PREFIX and len(pos) >= 3:\n",
    "                prefix_3_part = pos[:3]\n",
    "                candidates = get_valid_candidates_prefix(\n",
    "                    prefix_dict_3.get(prefix_3_part, []), pos, 3\n",
    "                )\n",
    "                all_candidates_pref.extend(candidates)\n",
    "            \n",
    "            # 3. Prefix_2\n",
    "            if len(all_candidates_pref) < NUM_NEG_SAMPLES_PREFIX and len(pos) >= 2:\n",
    "                prefix_2_part = pos[:2]\n",
    "                candidates = get_valid_candidates_prefix(\n",
    "                    prefix_dict_2.get(prefix_2_part, []), pos, 2\n",
    "                )\n",
    "                all_candidates_pref.extend(candidates)\n",
    "            \n",
    "            # 4. Prefix_1\n",
    "            if len(all_candidates_pref) < NUM_NEG_SAMPLES_PREFIX and len(pos) >= 1:\n",
    "                prefix_1_part = pos[:1]\n",
    "                candidates = get_valid_candidates_prefix(\n",
    "                    prefix_dict_1.get(prefix_1_part, []), pos, 1\n",
    "                )\n",
    "                all_candidates_pref.extend(candidates)\n",
    "            \n",
    "            # Select up to 2 prefix-based negatives\n",
    "            selected_prefix_neg = random.sample(\n",
    "                all_candidates_pref,\n",
    "                min(NUM_NEG_SAMPLES_PREFIX, len(all_candidates_pref))\n",
    "            )\n",
    "            prefix_neg_samples.extend(selected_prefix_neg)\n",
    "        \n",
    "        # ========== (c) RANDOM NEGATIVES TO REACH TOTAL 7 ==========\n",
    "        total_neg_samples = suffix_neg_samples + prefix_neg_samples\n",
    "        num_random_needed = max(0, TOTAL_NUM_NEGATIVES - len(total_neg_samples))\n",
    "        \n",
    "        random_neg_samples = []\n",
    "        if num_random_needed > 0:\n",
    "            valid_random_candidates = [x for x in all_positions_list if x not in total_neg_samples]\n",
    "            random_neg_samples = random.sample(\n",
    "                valid_random_candidates,\n",
    "                min(num_random_needed, len(valid_random_candidates))\n",
    "            )\n",
    "        \n",
    "        # Combine all three sets of negatives\n",
    "        neg_samples_all = total_neg_samples + random_neg_samples\n",
    "        \n",
    "        # Deduplicate, preserving order\n",
    "        seen = set()\n",
    "        neg_samples_deduped = []\n",
    "        for neg in neg_samples_all:\n",
    "            if neg not in seen:\n",
    "                seen.add(neg)\n",
    "                neg_samples_deduped.append(neg)\n",
    "        \n",
    "        entry['neg'] = neg_samples_deduped\n",
    "        updated_entries.append(entry)\n",
    "    \n",
    "    return updated_entries\n",
    "\n",
    "def main():\n",
    "    input_file = \"/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/all_triplets.jsonl\"\n",
    "    output_file = \"/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/all_triplets_4types_7neg.jsonl\"\n",
    "    \n",
    "    # Determine optimal chunk size based on file size\n",
    "    file_size = os.path.getsize(input_file)\n",
    "    chunk_size = max(1000, min(10000, file_size // (mp.cpu_count() * 1000000)))\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        results = []\n",
    "        for chunk_result in pool.imap_unordered(process_chunk, chunk_file(input_file, chunk_size)):\n",
    "            results.extend(chunk_result)\n",
    "    \n",
    "    # Write results\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for entry in results:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "    \n",
    "    print(f\"Updated entries written to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce7c10-af26-45e6-b6d9-4ed830ce4996",
   "metadata": {},
   "source": [
    "Set #negatives to 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20e5cb-6deb-41fa-8f95-9153fe5cf7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# File path to the output file\n",
    "file_path = \"/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/all_triplets_4types_7neg.jsonl\"\n",
    "\n",
    "# Counters for tracking\n",
    "deleted_entries_count = 0\n",
    "updated_entries = []\n",
    "\n",
    "# Process the file\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        entry = json.loads(line.strip())\n",
    "        negatives = entry.get('neg', [])\n",
    "\n",
    "        # If the number of negatives is less than 7, delete the entry\n",
    "        if len(negatives) < 7:\n",
    "            deleted_entries_count += 1\n",
    "            continue\n",
    "\n",
    "        # If the number of negatives is more than 7, truncate to 7\n",
    "        if len(negatives) > 7:\n",
    "            entry['neg'] = negatives[:7]\n",
    "\n",
    "        # Add the updated entry to the list\n",
    "        updated_entries.append(entry)\n",
    "\n",
    "# Write the updated entries back to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    for entry in updated_entries:\n",
    "        file.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "# Print the total number of deleted entries\n",
    "print(f\"Total number of deleted entries: {deleted_entries_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69417583-256a-45da-a902-2514f7b6ee0b",
   "metadata": {},
   "source": [
    "View JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a1644c-a4d5-47c7-9f42-4533964bd46c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSONL data from the file\n",
    "input_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/all_triplets_4types_7neg.jsonl'\n",
    "\n",
    "# Read all lines from the input file\n",
    "with open(input_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Print the length of the file\n",
    "print(f'Number of entries in the file: {len(lines)}')\n",
    "\n",
    "# Print the head of the file (first 5 entries)\n",
    "for i in range(min(200, len(lines))):\n",
    "    print(json.loads(lines[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0b110-8859-47a2-9de8-461fe6a131bd",
   "metadata": {},
   "source": [
    "top 200 most reccurent terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33dd005-49ba-421a-ad60-13c9a6b3d21d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Path to the JSONL file\n",
    "file_path = \"/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/all_triplets_4types_7neg.jsonl\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "else:\n",
    "    # Counter to store term frequencies\n",
    "    term_counter = Counter()\n",
    "\n",
    "    # Reading the JSONL file\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "            # Combine query and positive terms\n",
    "            terms = [data['query']] + data.get('pos', [])\n",
    "            # Update the term counter\n",
    "            term_counter.update(terms)\n",
    "\n",
    "    # Get the top 200 most frequent unique terms\n",
    "    top_200_terms = term_counter.most_common(200)\n",
    "\n",
    "    # Display the results\n",
    "    print(\"Top 200 most frequent unique terms in 'query' or 'pos':\")\n",
    "    for term, count in top_200_terms:\n",
    "        print(f\"{term}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
