{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the json extracted directly from clinicaltrials.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the input JSON file\n",
    "input_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-studies_523k_complete.json'\n",
    "output_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-studies_structured.json'\n",
    "\n",
    "with open(input_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(f\"Length of input file: {len(data)}\")\n",
    "\n",
    "# Parse the JSON data\n",
    "def parse_clinical_trials(data):\n",
    "    parsed_data = []\n",
    "    for entry in data:\n",
    "        protocol_section = entry.get(\"protocolSection\", {})\n",
    "        identification = protocol_section.get(\"identificationModule\", {})\n",
    "        description = protocol_section.get(\"descriptionModule\", {})\n",
    "        conditions = protocol_section.get(\"conditionsModule\", {}).get(\"conditions\", [])\n",
    "        keywords = protocol_section.get(\"conditionsModule\", {}).get(\"keywords\", [])\n",
    "        interventions = protocol_section.get(\"armsInterventionsModule\", {}).get(\"interventions\", [])\n",
    "        outcomes = protocol_section.get(\"outcomesModule\", {}).get(\"primaryOutcomes\", [])\n",
    "\n",
    "        # Extract intervention descriptions\n",
    "        intervention_descriptions = \"\"\n",
    "        for intervention in interventions:\n",
    "            name = intervention.get(\"name\", \"\")\n",
    "            desc = intervention.get(\"description\", \"\")\n",
    "            if name:\n",
    "                intervention_descriptions += f\"{name}: {desc}\\n\" if desc else f\"{name}\\n\"\n",
    "        intervention_descriptions = intervention_descriptions.strip()\n",
    "\n",
    "        parsed_entry = {\n",
    "            \"id\": identification.get(\"nctId\", \"\"),\n",
    "            \"title\": identification.get(\"officialTitle\") or identification.get(\"briefTitle\", \"\"),\n",
    "            \"summary\": description.get(\"briefSummary\", \"\").split('\\n\\n'),\n",
    "            \"condition\": conditions,\n",
    "            \"keywords\": keywords,\n",
    "            \"intervention\": [\n",
    "                intervention.get(\"name\", \"\")\n",
    "                for intervention in interventions\n",
    "                if intervention.get(\"type\") != \"OTHER\" and intervention.get(\"name\")\n",
    "            ],\n",
    "            \"intervention_description\": intervention_descriptions,\n",
    "            \"outcome_measure\": [outcome.get(\"measure\", \"\") for outcome in outcomes]\n",
    "        }\n",
    "\n",
    "        parsed_data.append(parsed_entry)\n",
    "\n",
    "    return parsed_data\n",
    "\n",
    "# Parse the data and write to output JSONL file\n",
    "parsed_data = parse_clinical_trials(data)\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    for entry in parsed_data:\n",
    "        json.dump(entry, file)\n",
    "        file.write('\\n')\n",
    "\n",
    "print(f\"Length of output file: {len(parsed_data)}\")\n",
    "print(f\"Parsed data saved to {output_file}\")\n",
    "\n",
    "# Read the output JSONL file and print the first two lines\n",
    "with open(output_file, 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        print(f\"Line {i+1}: {line.strip()}\")\n",
    "        if i == 20:  # Stop after printing two lines\n",
    "            break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the parsed JSON file\n",
    "input_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-studies_structured.json'\n",
    "output_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-triplets.json'\n",
    "\n",
    "# Create an empty list to store the resulting triplets\n",
    "triplets = []\n",
    "\n",
    "# Read and process each line of the JSONL file\n",
    "with open(input_file, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        entry = json.loads(line)\n",
    "        \n",
    "        # Extract fields\n",
    "        title = entry.get(\"title\", \"\").strip()\n",
    "        summary = [s.strip() for s in entry.get(\"summary\", []) if s.strip()]\n",
    "        keywords = [k.strip() for k in entry.get(\"keywords\", []) if k.strip()]\n",
    "        condition = [c.strip() for c in entry.get(\"condition\", []) if c.strip()]\n",
    "        intervention = [i.strip() for i in entry.get(\"intervention\", []) if i.strip()]\n",
    "        intervention_description = entry.get(\"intervention_description\", \"\").strip()\n",
    "        outcome_measure = [o.strip() for o in entry.get(\"outcome_measure\", []) if o.strip()]\n",
    "        \n",
    "\n",
    "        # Create triplets: each element of condition with each element of intervention\n",
    "        for cond in condition:\n",
    "            for interv in intervention:\n",
    "                if cond and interv:\n",
    "                    triplet = {\n",
    "                        \"query\": cond,\n",
    "                        \"pos\": [interv],\n",
    "                        \"neg\": [\"\"],\n",
    "                        \"category\": \"intervention\"\n",
    "                    }\n",
    "                    triplets.append(triplet)\n",
    "    \n",
    "        # Create triplets: each element of condition with each element of keywords\n",
    "        for cond in condition:\n",
    "            for key in keywords:\n",
    "                if cond and key:\n",
    "                    triplet = {\n",
    "                        \"query\": cond,\n",
    "                        \"pos\": [key],\n",
    "                        \"neg\": [\"\"],\n",
    "                        \"category\": \"keyword\"\n",
    "                    }\n",
    "                    triplets.append(triplet)\n",
    "\n",
    "\"\"\"       # Create triplets: intervention_description with each element of outcome_measure \n",
    "        for out in outcome_measure:\n",
    "            if intervention_description and out:\n",
    "                triplet = {\n",
    "                    \"query\": intervention_description,\n",
    "                    \"pos\": [out],\n",
    "                    \"neg\": [\"\"],\n",
    "                    \"category\": \"outcome\"\n",
    "                }\n",
    "                triplets.append(triplet)\n",
    "\n",
    "        # Create triplets: title with each element of summary\n",
    "        for summ in summary:\n",
    "            if title and summ:\n",
    "                triplet = {\n",
    "                    \"query\": title,\n",
    "                    \"pos\": [summ],\n",
    "                    \"neg\": [\"\"],\n",
    "                    \"category\": \"summary\"\n",
    "                }\n",
    "                triplets.append(triplet)\n",
    " \"\"\"      \n",
    "            \n",
    "# Write the resulting triplets to an output JSONL file\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    for triplet in triplets:\n",
    "        outfile.write(json.dumps(triplet) + \"\\n\")\n",
    "\n",
    "# Print the first ten entries of the resulting JSONL\n",
    "for i in range(min(10, len(triplets))):\n",
    "    print(json.dumps(triplets[i], indent=2))\n",
    "\n",
    "# Print the length of the resulting JSONL file\n",
    "print(f\"Total number of triplets: {len(triplets)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "# Define the cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Load the JSONL data from the file\n",
    "input_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-triplets.json'\n",
    "output_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-triplets.jsonl'\n",
    "\n",
    "cleaned_entries = []\n",
    "\n",
    "with open(input_file, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():  # Skip empty lines\n",
    "            entry = json.loads(line)\n",
    "            # Clean the 'query', 'pos', and 'neg' fields\n",
    "            entry['query'] = clean_text(entry['query'])\n",
    "            entry['pos'] = [clean_text(text) for text in entry['pos']]\n",
    "           # entry['neg'] = [clean_text(text) for text in entry['neg']]\n",
    "            cleaned_entries.append(entry)\n",
    "\n",
    "# Save the cleaned data to a new JSONL file\n",
    "with open(output_file, 'w') as f:\n",
    "    for entry in cleaned_entries:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f'Cleaned data saved to {output_file}')\n",
    "\n",
    "# Delete the specified file\n",
    "file_to_delete = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-triplets.json'\n",
    "if os.path.exists(file_to_delete):\n",
    "    os.remove(file_to_delete)\n",
    "    print(f\"Deleted file: {file_to_delete}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_to_delete}\")\n",
    "\n",
    "# Delete the specified file\n",
    "file_to_delete = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-studies_structured.json'\n",
    "if os.path.exists(file_to_delete):\n",
    "    os.remove(file_to_delete)\n",
    "    print(f\"Deleted file: {file_to_delete}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_to_delete}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVE DOUBLONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-triplets.jsonl'\n",
    "\n",
    "def modify_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    modified_lines = []\n",
    "    for line in lines:\n",
    "        entry = json.loads(line)\n",
    "        if entry['query'].lower() not in [pos.lower() for pos in entry['pos'] if pos]:\n",
    "            modified_lines.append(json.dumps(entry) + '\\n')\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.writelines(modified_lines)\n",
    "\n",
    "modify_file(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtersize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "input_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-triplets_v3.2.jsonl'\n",
    "output_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/SMALL_ctg-triplets_v3.2.jsonl'\n",
    "\n",
    "# Set the limit of entries to extract\n",
    "limit = 100000\n",
    "\n",
    "# Read all lines from the input file\n",
    "with open(input_file, 'r') as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "# Shuffle the lines randomly\n",
    "random.shuffle(lines)\n",
    "\n",
    "# Select the first 'limit' entries\n",
    "selected_lines = lines[:limit]\n",
    "\n",
    "# Write the selected entries to the output file\n",
    "with open(output_file, 'w') as outfile:\n",
    "    for line in selected_lines:\n",
    "        outfile.write(line)\n",
    "\n",
    "print(f\"Extracted {len(selected_lines)} entries to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILTER  MAX Token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the JSONL data from the file\n",
    "input_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-triplets_v3.jsonl'\n",
    "output_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-triplets_v3.jsonl'\n",
    "\n",
    "# Parameters for token length filtering\n",
    "max_query_tokens = 256\n",
    "max_pos_tokens = 256\n",
    "batch_size = 1024\n",
    "\n",
    "# Counter for removed triplets\n",
    "removed_triplets_count = 0\n",
    "\n",
    "def filter_batch(batch_data):\n",
    "    global removed_triplets_count\n",
    "    filtered_data = []\n",
    "    for entry in batch_data:\n",
    "        entry = json.loads(entry)  # Convert string to dictionary\n",
    "        query_tokens = len(tokenizer.tokenize(entry.get(\"query\", \"\")))\n",
    "        pos_tokens_list = [len(tokenizer.tokenize(pos)) for pos in entry.get(\"pos\", [])]\n",
    "\n",
    "        if query_tokens <= max_query_tokens and all(pos_tokens <= max_pos_tokens for pos_tokens in pos_tokens_list):\n",
    "            filtered_data.append(entry)\n",
    "        else:\n",
    "            removed_triplets_count += 1\n",
    "    return filtered_data\n",
    "\n",
    "class TripletsDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                self.data.append(line.strip())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Using PyTorch DataLoader to batch process the data\n",
    "dataset = TripletsDataset(input_file)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "\n",
    "filtered_data = []\n",
    "max_query_length = 0\n",
    "max_pos_length = 0\n",
    "\n",
    "for batch in data_loader:\n",
    "    batch_filtered = filter_batch(batch)\n",
    "    filtered_data.extend(batch_filtered)\n",
    "\n",
    "    # Calculate max token length for the current batch\n",
    "    for entry in batch_filtered:\n",
    "        query_length = len(tokenizer.tokenize(entry.get(\"query\", \"\")))\n",
    "        pos_lengths = [len(tokenizer.tokenize(pos)) for pos in entry.get(\"pos\", [])]\n",
    "\n",
    "        max_query_length = max(max_query_length, query_length)\n",
    "        if pos_lengths:\n",
    "            max_pos_length = max(max_pos_length, max(pos_lengths))\n",
    "\n",
    "# Write the filtered data to output JSONL file\n",
    "with open(output_file, 'w') as file:\n",
    "    for entry in filtered_data:\n",
    "        file.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(f\"Filtered triplets data saved to {output_file}\")\n",
    "print(f\"Length of final filtered data: {len(filtered_data)}\")\n",
    "print(f\"Maximum query token length in filtered data: {max_query_length}\")\n",
    "print(f\"Maximum positive passage token length in filtered data: {max_pos_length}\")\n",
    "print(f\"Number of triplets removed due to exceeding token limits: {removed_triplets_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIEW JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSONL data from the file\n",
    "input_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-triplets.jsonl'\n",
    "\n",
    "# Read all lines from the input file\n",
    "with open(input_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Print the length of the file\n",
    "print(f'Number of entries in the file: {len(lines)}')\n",
    "\n",
    "# Print the head of the file (first 5 entries)\n",
    "for i in range(min(100, len(lines))):\n",
    "    print(json.loads(lines[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIEW JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the file\n",
    "input_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/unique_training_terms.json'\n",
    "\n",
    "# Read and parse the JSON file\n",
    "with open(input_file, 'r') as f:\n",
    "    data = json.load(f)  # Load as a single JSON object (list or dict)\n",
    "\n",
    "# Print the number of entries if it's a list\n",
    "if isinstance(data, list):\n",
    "    print(f'Number of entries in the JSON file: {len(data)}')\n",
    "\n",
    "# Print the head of the data (first 5 entries for a list, or the first 5 keys for a dictionary)\n",
    "if isinstance(data, list):\n",
    "    for i in range(min(5, len(data))):\n",
    "        print(data[i])\n",
    "elif isinstance(data, dict):\n",
    "    keys = list(data.keys())\n",
    "    for key in keys[:5]:\n",
    "        print(f'{key}: {data[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of unique training terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "input_file = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-triplets_v3.2.jsonl'\n",
    "\n",
    "# Read all lines from the input file\n",
    "with open(input_file, 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# Extract the terms from the 'query' and 'pos' fields\n",
    "terms = set()\n",
    "\n",
    "# Iterate through each line, parse it, and extract the terms\n",
    "for line in data:\n",
    "    try:\n",
    "        record = json.loads(line)\n",
    "        # Split the 'query' string into words and add them to the set\n",
    "        terms.update(record['query'].split())\n",
    "\n",
    "        # Loop through 'pos' list, split each string into words, and add to the set\n",
    "        for item in record['pos']:\n",
    "            terms.update(item.split())\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error parsing line: {line}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key {e} in line: {line}\")\n",
    "\n",
    "# Remove any empty strings that might have been added\n",
    "terms.discard('')\n",
    "\n",
    "# Write the output JSON to a file\n",
    "output_file = '/n/data1/hsph/biostat/celehs/lab/jh537/Retrivial_task/DATA/unique_ct_terms_weight.json'\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(list(terms), file, indent=4)\n",
    "\n",
    "print(\"Unique terms have been written to unique_training_terms.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Query Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Path to the JSONL file\n",
    "file_path = \"/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/Train/ctg-triplets_v3.2.jsonl\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "else:\n",
    "    # Counter to store term frequencies\n",
    "    term_counter = Counter()\n",
    "\n",
    "    # Reading the JSONL file\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "            # Combine query and positive terms\n",
    "            terms = [data['query']] + data.get('pos', [])\n",
    "            # Update the term counter\n",
    "            term_counter.update(terms)\n",
    "\n",
    "    # Get the top 200 most frequent unique terms\n",
    "    top_200_terms = term_counter.most_common(200)\n",
    "\n",
    "    # Display the results\n",
    "    print(\"Top 200 most frequent unique terms in 'query' or 'pos':\")\n",
    "    for term, count in top_200_terms:\n",
    "        print(f\"{term}: {count}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform to good shape (from json to jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# Paths for input and output files\n",
    "input_file = ''\n",
    "output_file = ''\n",
    "\n",
    "\n",
    "# Load the input JSONL file and transform the data\n",
    "with open(input_file, 'r') as infile:\n",
    "    data = json.load(infile)  # Load the entire JSON file\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for trial in data:\n",
    "            transformed_entry = {\n",
    "                \"query\": trial[\"query\"],\n",
    "                \"pos\": trial[\"pos\"],\n",
    "                \"neg\": trial[\"neg\"],\n",
    "                \"category\":tiral[\"category\"]\n",
    "            }\n",
    "            outfile.write(json.dumps(transformed_entry) + '\\n')\n",
    "       \n",
    "print(f\"Transformation complete. Transformed data saved to {output_file}\")\n",
    "\n",
    "# Delete the specified file\n",
    "file_to_delete = '/home/jh537/Clinical_Trial_Embending/Clinical_Trial_data/Clinical_Trial_Triplet_v3/ctg-triplets_v3.json'\n",
    "if os.path.exists(file_to_delete):\n",
    "    os.remove(file_to_delete)\n",
    "    print(f\"Deleted file: {file_to_delete}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_to_delete}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
